<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>این درس را مشترکا با دکتر علیشاهی ارائه می‌کنیم. قرار است در این درس ابتدا کمی بهینه‌سازی مقدماتی درس بدهیم، کمی برنامه‌ریزی خطی و کمی بهینه‌سازی محدب. که احتمالا حدود ۳ هفته این طول بکشد. بعد مقداری بهینه‌سازی برخط، بعد هم کمی یادگیری، بعد هم کمی یادگیری برخط و بعد هم کمی تلاش می‌کنیم چند مساله حل کنیم با این روش‌ها. دو سه کتاب نامزد کتاب درس شدن هستند. </p> <ul> <li> <a href="http://sbubeck.com/BubeckLectureNotes.pdf" rel="external nofollow noopener" target="_blank">Introduction to Online Optimization</a> که جزوه‌های درس آقای Bubeck است در سال ۲۰۱۱.</li> <li> <a href="http://ocobook.cs.princeton.edu/OCObook.pdf" rel="external nofollow noopener" target="_blank">Introduction to Online Convex Optimization</a> کتاب آقای Hazan.</li> <li> <a href="https://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf" rel="external nofollow noopener" target="_blank">Online Learning and Online Convex Optimization</a>.</li> </ul> <p>مباحث گفته شده در کتاب‌های بالا هستند. می‌توانید نگاه کنید و پیشاپیش از درس لذت ببرید. امیدوارم در ترم آینده بیشتر من این درس را درس بدهم و دکتر علیشاهی بیشتر شنونده باشند.</p> <p>طبیعتا درس کمی طعم ریاضی خواهد داشت و کم‌تر طعم الگوریتمی. از درس بهینه‌سازی محدب کم‌تر و از درس‌های ارشدی که معمولا من درس می‌دهم بیشتر ریاضی‌گونه خواهد بود.</p> <p>به عنوان پیش‌نیاز الگوریتم و ریاضی ۲ که لازم هستند. دانستن آمار موجب می‌شود مثال‌ها را خیلی بهتر متوجه شوید.</p> <p>توصیف درس در قالب فایل پی‌دی‌اف در <a href="http://old.foroughmand.ir/wp-content/uploads/courses/971/opt4ds-971/opt4ds-desc-971.pdf" rel="external nofollow noopener" target="_blank">این فایل</a> آمده است. به نظر می‌آید از کتاب آقای بوبک در این درس استفاده نکنیم.</p> <p>برخی منابع مرتبط</p> <ul> <li> <a href="http://research.microsoft.com/en-us/um/people/sebubeck/SurveyBCB12.pdf" rel="external nofollow noopener" target="_blank">Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems</a> S. Bubeck and N. Cesa-Bianchi</li> <li> <a href="http://theoryofcomputing.org/articles/v008a006/" rel="external nofollow noopener" target="_blank">The Multiplicative Weights Update Method: a Meta-Algorithm and Applications</a> S. Arora, E. Hazan and S. Kale </li> <li> <a href="http://www.cs.cornell.edu/~sridharan/lecnotes.pdf" rel="external nofollow noopener" target="_blank">Statistical Learning and Sequential Prediction</a> A. Rakhlin and K. Sridharan </li> <li> <a href="http://www.cs.princeton.edu/~ehazan/papers/OCO-survey.pdf" rel="external nofollow noopener" target="_blank">The convex optimization approach to regret minimization</a> E. Hazan</li> <li> <p>Bandit Algorithms Book</p> </li> <li><a href="https://courses.cs.washington.edu/courses/cse599i/18wi/" rel="external nofollow noopener" target="_blank">Online and Adaptive Methods for Machine Learning</a></li> <li> <a href="http://theoryofcomputing.org/articles/v008a006/" rel="external nofollow noopener" target="_blank">The Multiplicative Weights Update Method: a Meta-Algorithm and Applications</a> S. Arora, E. Hazan and S. Kale </li> <li> <a href="http://www.cs.cornell.edu/~sridharan/lecnotes.pdf" rel="external nofollow noopener" target="_blank">Statistical Learning and Sequential Prediction</a> A. Rakhlin and K. Sridharan </li> <li> <p><a href="http://www.cs.princeton.edu/~ehazan/papers/OCO-survey.pdf" rel="external nofollow noopener" target="_blank">The convex optimization approach to regret minimization</a> E. Hazan </p> </li> <li><a href="http://www-bcf.usc.edu/~haipengl/courses/CSCI699//" rel="external nofollow noopener" target="_blank">Introduction to Online Learning</a></li> <li><a href="http://www.cs.cornell.edu/courses/cs6783/2015fa/" rel="external nofollow noopener" target="_blank">Machine Learning Theory</a></li> <li> <p><a href="https://courses.cs.washington.edu/courses/cse599s/14sp/index.html" rel="external nofollow noopener" target="_blank">Online Learning</a></p> </li> <li><a href="http://web.eecs.umich.edu/~jabernet/eecs598course/fall2013/web/" rel="external nofollow noopener" target="_blank">Prediction and Learning: It’s Only a Game</a></li> <li><a href="https://www.cse.iitk.ac.in/users/purushot/courses/olo/2015-16-w/index.php" rel="external nofollow noopener" target="_blank">Online Learning and Optimization</a></li> <li><a href="https://blogs.princeton.edu/imabandit/orf523-the-complexities-of-optimization/" rel="external nofollow noopener" target="_blank">The complexities of optimization</a></li> <li>کتاب‌چه‌ای برای درس <a href="http://www.mit.edu/~rakhlin/courses/stat928/stat928_notes.pdf" rel="external nofollow noopener" target="_blank">Statistical Learning and Sequential Prediction</a> </li> <li> <p>مطالب یک درس <a href="http://www.mit.edu/~rakhlin/6.883/" rel="external nofollow noopener" target="_blank">Online Methods in Machine Learning</a></p> </li> <li>شاید این از همه درس‌های قبلی بهتر باشد <a href="http://www.ece.iisc.ernet.in/~aditya/E1245_F15.html" rel="external nofollow noopener" target="_blank">Online Prediction and Learning</a> </li> <li>اما این منبع از همه بهتر است: <a href="https://homes.cs.washington.edu/~jrl/teaching/cse599I-spring-2018/" rel="external nofollow noopener" target="_blank">Competitive analysis via convex optimization</a>. اما ای‌کاش همه جزوه‌هایش موجود بود.</li> <li>مقاله‌ای جمع‌بندی در مورد کارهای انجام شده در یادگیری برخط <a href="https://arxiv.org/pdf/1802.02871.pdf" rel="external nofollow noopener" target="_blank">Online Learning: A Comprehensive Survey</a> </li> <li> <a href="http://www-bcf.usc.edu/~haipengl/courses/CSCI699//" rel="external nofollow noopener" target="_blank">درس مقدمه‌ای بر یادگیری برخط</a> </li> <li>یک کتاب جدیدتر: <a href="https://arxiv.org/abs/1904.07272" rel="external nofollow noopener" target="_blank">Introduction to Multi-Armed Bandits</a> </li> </ul> <h4 id="پسنوشت-۲۰-شهریور">پس‌نوشت (۲۰ شهریور)</h4> <p>پس از مذاکرات طولانی با دکتر علیشاهی، قرار شد ایشان ابتدای درس را درس بدهند. احتمالا ابتدای درس را از روی کتاب آقای هیزن درس می‌دهند. هنوز نمی‌دانیم این قسمت چقدر طول می‌کشد. احتمالا خیلی طول نخواهد کشید، یک چیزی حدود یک و نیم ماه. بعدش هم احتمالا جنبه‌های یادگیری ماشین را بیشتر مطرح کنیم. مثلا یک مقاله هست به نام Online Learning: Beyond Regret که شاید از روی مقاله و مقاله‌های پیش از آن درس را ادامه بدهیم.</p> <h4 id="پروژه-درس">پروژه درس</h4> <p>درس آقای لوو چند نوع پروژه دارد. یکی از آن انواع را برای پروژه درس انتخاب کردیم. در این پروژه چند الگوریتم یادگیری برخط را برروی یک سری داده اجرا می‌کنیم. فعلا داده‌های انتخاب شده داده مربوط به سرطان روده بزرگ با مشخصات زیر است:</p> <p>Source: [<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ref.html#AU99a" rel="external nofollow noopener" target="_blank">AU99a</a>]</p> <p>Preprocessing: Instance-wise normalization to mean zero and variance one. Then feature-wise normalization to mean zero and variance one. [<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ref.html#SKS03a" rel="external nofollow noopener" target="_blank">SKS03a</a>]</p> <p># of classes: 2</p> <p># of data: 62</p> <p># of features: 2,000</p> <p>Files: <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/colon-cancer.bz2" rel="external nofollow noopener" target="_blank">colon-cancer.bz2</a></p> <p>یک مجموعه داده: <a href="https://archive.ics.uci.edu/ml/datasets.html" rel="external nofollow noopener" target="_blank">UCI</a> و یک <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/" rel="external nofollow noopener" target="_blank">مجموعه داده دیگر</a>.</p> <p>در داده بالا، باید اولین عدد را که یک یا منهای یک است بر اساس ستون‌های دیگر پیش‌بینی کنید. عدد ابتدایی در مورد سرطانی بودن است و اعداد ستون‌های دیگر میزان بیان ژن را مشخص می‌کند.</p> <p>شما باید فرض کنید که داده‌ها یکی یکی می‌آیند. برای هرکدام شما باید هزینه انتخاب خود را لحاظ کنید و پارامترهای الگوریتم را به‌روز کرده و منتظر داده بعدی باشید. در نهایت میزان حسرت الگوریتم شما خوبی الگوریتمتان را می رساند.</p> <p>الگوریتم اول برای آزمایش، الگوریتم مشاوران است. در این الگوریتم فرض کنید هر کدام از مشاوران یک ژن را انتخاب می‌کنند و شما می‌خواهید با الگوریتم هدج روشی پیشه کنید که از بهترین مشاوران خیلی بدتر نباشید.</p> <p>الگوریتم دوم، الگوریتم Squint است.</p> <p>در الگوریتم‌های بالا سعی کنید حساسیت الگوریتم خود را به ترتیب ورودی‌ها محاسبه کنید. همچنین سعی کنید پارامترهای اولیه را تغییر دهید و کارآیی الگوریتم خود را با این روش محاسبه کنید.</p> <h4 id="آزمونها">آزمون‌ها</h4> <ul> <li>آزمون پایان‌ترم</li> </ul> <p>سوال ۶ از آزمون پایان‌ترم براساس قسمت ابتدایی از جلسه ۱۱ از درس آقای لوو طراحی شده. می‌توانید راه حل آن را <a href="https://haipeng-luo.net/courses/CSCI699/lecture11.pdf" rel="external nofollow noopener" target="_blank">اینجا</a> بیابید.</p> </body></html>