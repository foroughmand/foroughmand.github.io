---
layout: post
title: "ترنسفورمرها، منابعی برای یادگیری‎"
date: 2024-02-08 00:00:00
description: در حال آشنایی اولیه با ترنسفورمرها هستم. ترنسفورمرها، یک معماری خاص برای استفاده از شبکه‌های عصبی در یادگیری ماشین ارائه می‌دهند که به طرز شگفت‌آوری خوب کار می‌کنند. یکی از ابزارهایی که با ترنسفورمرها ساخته شده و جدیدا مورد توجه قرارگرفته ChatGPT و دوستانش هستند.
tags: هوش-مصنوعی ترانسفورمر
categories: هوش۰مصنوعی
---

در حال آشنایی اولیه با ترنسفورمرها هستم. ترنسفورمرها، یک معماری خاص برای استفاده از شبکه‌های عصبی در یادگیری ماشین ارائه می‌دهند که به طرز شگفت‌آوری خوب کار می‌کنند. یکی از ابزارهایی که با ترنسفورمرها ساخته شده و جدیدا مورد توجه قرارگرفته ChatGPT و دوستانش هستند.

ترنسفورمرها ساختارهای بسیار پیچیده‌ای دارند برای همین شاید یادگرفتن آن‌ها بدیهی نباشد. اینجا برخی منابع برای یادگرفتن ترنسفورمرها را معرفی می‌کنیم:


* [فیلم‌های آموزشی آکادمی سرانو](https://www.youtube.com/watch?v=OxCpWwDCDFQ&list=PLs8w1Cdi-zvYskDS2icIItfZgxclApVLv) شامل مفاهیم اولیه و توضیحات نسبتا روانی از شبکه‌های عصبی و لایه توجه و ترنسفورمرهاست.

* [کتاب Natural Language Processing with Transformers](https://www.google.de/books/edition/Natural_Language_Processing_with_Transfo/nTxbEAAAQBAJ) که علاوه بر آموزش کلیات، کتابخانه huggingface را هم آموزش می‌دهد که ابزار ساده‌ای برای کارکردن با ترنسفورمرها و داده‌هاست.

* برای رشته‌های طولانی، ظاهرا کارکردن با ترنسفورمرها مشکلاتی جدی دارد. مخصوصا که برای کاربردهای بیوانفورماتیکی نیاز دارید که از ترنسفورمرهایی استفاده کنیم که رشته‌های طولانی را دریافت می‌کنند. در مدیوم [مقاله‌ای](https://medium.com/@lukas.noebauer/the-big-picture-transformers-for-long-sequences-890cc0e7613b) در این مورد هست. وبگاه huggingface هم [مقاله](https://huggingface.co/blog/long-range-transformers) خوبی در این زمینه دارد.

برخی جزئیات در مورد اینکه محاسبات مراحل مختلف ترنسفورمرها چگونه انجام می‌شود در [اینجا (لایه دگرنمایی)](https://medium.com/@hunter-j-phillips/the-embedding-layer-27d9c980d124) و [اینجا (رمزگذاری مکانی)](https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6) و [اینجا (لایه توجه چندسر)](https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a) آمده است.




